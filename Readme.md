# serverless-workflow-operator-subflows

Simple example that shows how a `master` workflow can execute workflows `workflowA`, `workflowB` and `workflowC` as subflows.

General comments:

* Every subflow, when called receives a clone of the `master` workflow data.
* We can use different alternatives to make the `master` workflow receive data produced by the subflows when they finish their respective executions. 
* The subflows can also use data input schemas for validating their respective input.
* Sometimes, required data by a subflow can be produced by master.
* Sometimes, the `master` workflow can adjust its own data input schema to ensure required data by a subflow is present when the `master` starts.
* If the data input schema (if present) for a subflow is not validated when it's called, then, the subflow will produce a validation error, that will make the `master` workflow also fail at the given invocation point. 



## To execute the example in standalone mode do:

````
mvn quarkus:dev
````

In a separate terminal do:

````
curl --location 'http://localhost:8080/master' \
--header 'Content-Type: application/json' \
--data '{ "paramA1" : "paramA1 is provided by the caller" }'
````

You will see an output like this:
````
{
  "id": "987951c2-8b5d-4111-b241-bd7b96a5ee6f",
  "workflowdata": {
    "paramA1": "paramA1 is provided by the caller",
    "paramB1": "Param required by workflowB produced by an internal master workflow calculation",
    "filteredValueC2": "Value C2 generated by worfklowC",
    "valueB1": "Value B1 generated by workflowB",
    "valueB2": "Value B2 generated by worfklowB"
  }
}
````

## To build and execute the image do:

Make sure you adjust the REGISTRY_USER and REGISTRY variables if needed, see the scripts.

````
./build-image-1.36.0.sh
````

````
./start-application-image-1.36.0.sh
````

In a separate terminal do:

````
curl --location 'http://localhost:8080/master' \
--header 'Content-Type: application/json' \
--data '{ "paramA1" : "paramA1 is provided by the caller" }'
````


## To execute the workflow in OpenShift do

1. Build the image and make sure you published it to the corresponding registry 

2. Make sure that you adjust the SonataFlow `kubernetes/04-sonataflow_master.yaml` to use the correct image

````
podTemplate:
  container:
    image: your-registry/your-user/serverless-workflow-operator-subflows:1.0
````
3. Create the `subflows-example`
````
oc create namespace subflows-example
````
4. Deploy the workflow and give it some time to start, etc.
````
oc kustomize 1.36.0/kubernetes | oc apply -f - -n subflows-example
````

5. Expose the service for local testing and execute (never in production OCP)

````
oc expose service master -n subflows-example
````

````
SERVICE_URL="http://$(oc get route master -n subflows-example  -o jsonpath='{.spec.host}')/master"; echo $SERVICE_URL
````


````
curl --location $SERVICE_URL \
--header 'Content-Type: application/json' \
--data '{ "paramA1" : "paramA1 is provided by the caller" }'
````

Alternatively you can just execute the following command in the corresponding pod terminal, etc.

````
curl --location 'http://localhost:8080/master' \
--header 'Content-Type: application/json' \
--data '{ "paramA1" : "paramA1 is provided by the caller" }'
````

